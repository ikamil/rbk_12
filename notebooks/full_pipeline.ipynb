{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "organic-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from definitions import INPUT_DATA_DIR, PARSED_DATA_DIR, SUB_DIR, FULL_DATA_DIR, INPUT_DATA_DIR, AUTHORS_DIR\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electoral-branch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lightgbm': '3.3.2', 'pandas': '1.4.3', 'numpy': '1.23.1', 'scikit-learn': '1.1.1'}\n"
     ]
    }
   ],
   "source": [
    "version = {\n",
    "    'lightgbm': lgb.__version__,\n",
    "    'pandas': pd.__version__,\n",
    "    'numpy': np.__version__,\n",
    "    'scikit-learn': sk.__version__ \n",
    "}\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-jerusalem",
   "metadata": {},
   "source": [
    "# 1. Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accredited-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformers.base import Compose\n",
    "from src.transformers.preprocess import (\n",
    "    LoaderMergePreprocess, \n",
    "    CategoryFromTextPreprocess, \n",
    "    AuthorsPreprocess,\n",
    "    TagsPreprocess,\n",
    "    FeaturePreprocess,\n",
    "    SaverPreprocess,\n",
    "    NatashaTransformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "republican-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessor = Compose(\n",
    "    transforms=[\n",
    "        LoaderMergePreprocess(name='loading'),\n",
    "        CategoryFromTextPreprocess(name='category from text'),\n",
    "        AuthorsPreprocess(name='authors preprocess'),\n",
    "        TagsPreprocess(name='tags preprocess'),\n",
    "        FeaturePreprocess(name='feature selector'),\n",
    "        NatashaTransformer(name='natasha name entity'),\n",
    "        SaverPreprocess(name='saving files')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-purchase",
   "metadata": {},
   "source": [
    "Данный код запускет `preprocessing` данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "artistic-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 7000/7000 [01:50<00:00, 63.24it/s]\n",
      "test: 100%|████████████████████████████████████████████████████████████████████████| 3000/3000 [00:52<00:00, 57.53it/s]\n"
     ]
    }
   ],
   "source": [
    "train = Preprocessor(data=pd.DataFrame(), mode='train')\n",
    "test = Preprocessor(data=pd.DataFrame(), mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-shopping",
   "metadata": {},
   "source": [
    "Либо можно их сразу загрузить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "honest-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(FULL_DATA_DIR / 'full_train.json')\n",
    "test = pd.read_json(FULL_DATA_DIR / 'full_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "numerous-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['NAMES_FROM_TITLE'] = (\n",
    "    train['ORG_TITLE'].apply(lambda x: x if isinstance(x, list) else []) + \n",
    "    train['LOC_TITLE'].apply(lambda x: x if isinstance(x, list) else []) + \n",
    "    train['PER_TITLE'].apply(lambda x: x if isinstance(x, list) else [])\n",
    ")\n",
    "test['NAMES_FROM_TITLE'] = (\n",
    "    test['ORG_TITLE'].apply(lambda x: x if isinstance(x, list) else []) + \n",
    "    test['LOC_TITLE'].apply(lambda x: x if isinstance(x, list) else []) + \n",
    "    test['PER_TITLE'].apply(lambda x: x if isinstance(x, list) else [])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-hormone",
   "metadata": {},
   "source": [
    "## 2. Предсказание для существующих"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bored-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prediction.utils import predict_for_exist\n",
    "from src.constants import FeaturesConstants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aboriginal-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "exist_prediction = predict_for_exist(train, test)\n",
    "train_and_tests_ctr_ = list(set(train['page_id']) & set(test['page_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sound-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (\n",
    "    train[~(train['page_id'].isin(train_and_tests_ctr_))]\n",
    "    .drop_duplicates(FeaturesConstants.target + ['document_id', 'page_id', 'ctr'])\n",
    ")\n",
    "train_data = train_data[train_data['full_reads_percent'] <= 100]\n",
    "train_data = train_data[pd.to_datetime(train_data['publish_date']).dt.year >= 2022]\n",
    "train_data = train_data[train_data['views'] <= 1000000].copy()\n",
    "\n",
    "test_data = test[~(test['page_id'].isin(train_and_tests_ctr_))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "respected-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = train_data[FeaturesConstants.target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "first-donor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6912, 37), (2967, 34))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-panel",
   "metadata": {},
   "source": [
    "## 3. Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-northeast",
   "metadata": {},
   "source": [
    "## 3.1 Identifier\n",
    "\n",
    "- `page_id`: `624ac09c9a7947db3d80c98e`\n",
    "\n",
    "## 3.2 Date\n",
    "\n",
    "- `publish_date`: `2022-04-04 10:29:44` + `timedelta(hours=3)`\n",
    "\n",
    "## 3.3 Title\n",
    "\n",
    "- `title` - title новости от организаторов, (**текст**)\n",
    "- `title_parsed` - спарсенный title новости, (**текст**)\n",
    "- `optional[title_parsed_from_yandex]` - заголовок статьи в `yandex`, (**текст**)\n",
    "- `title_preprocessed` - удалены `category`\n",
    "\n",
    "## 3.4 Category\n",
    "\n",
    "- `category` - категория новости, **hash**\n",
    "- `category_parsed` - спарсенная категория новости, **текст**\n",
    "- `category_from_title` - категория, выделенная из текста, **Optional[текст]**\n",
    "\n",
    "## 3.5 Ctr\n",
    "\n",
    "- `ctr` - показатель кликабельности, **Optional[float64]**\n",
    "\n",
    "## 3.6 Tags\n",
    "\n",
    "- `tags` - таги, закодированные от организаторов, **Optional[hash]**\n",
    "- `tags_parsed` - спарсенные тэги, **Optional[текст]**\n",
    "\n",
    "## 3.7 Auhtors\n",
    "\n",
    "- `authors` - авторы новости от организаторов, **Optional[hash]**\n",
    "- `authors_parsed` - авторы новости **Optional[текст]**\n",
    "\n",
    "## 3.8 Text\n",
    "\n",
    "- `news_text_parsed` - текст новости, полный, **текст** \n",
    "- `news_text_overview_parsed` - некоторая выжимка новости, **текст**\n",
    "- `news_amount_of_paragraphs_parsed` - количество параграфов в тексте, **int64**\n",
    "- `news_amount_of_inline_items_parsed` - количество ссылок на другие новости в тексте новости, **int64**\n",
    "- `news_inline_titles_parsed` - заголовки на другие статьи в тексте новости, **Optional[текст]**\n",
    "- `news_has_image_parsed` - есть ли в новости картинка, **int64**\n",
    "- `news_image_title_parsed` - подпись к изображению, если есть, **Optional[текст]**\n",
    "\n",
    "## 3.9. Features Selector Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tender-bidder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shiro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.transformers.feature_generation import (\n",
    "    DatetimeTransformer,\n",
    "    TitleTransformer,\n",
    "    CategoryTransformer,\n",
    "    CTRTransformer,\n",
    "    TagsTransformer,\n",
    "    AuthorsTransformer,\n",
    "    TextTransformer,\n",
    "    NatashaTextTransformer,\n",
    "    FeatureSelector,\n",
    "    TfidfVectorTransformer,\n",
    "    CountTfIdfVectorizer,\n",
    "    MultiLabelTransformer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-collins",
   "metadata": {},
   "source": [
    "# 4. Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "exterior-capitol",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiro\\Desktop\\DATABASE\\competitions\\summer_2022\\rbk\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfTransformer,\n",
    "    TfidfVectorizer,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "standard-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImp(\n",
    "    model,\n",
    "    X, \n",
    "    num=10, \n",
    "    fig_size = (40, 20)\n",
    "):\n",
    "    feature_imp = pd.DataFrame(\n",
    "        {'Value': model.feature_importances_,\n",
    "         'Feature': X.columns}\n",
    "    )\n",
    "    fig, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "    sns.barplot(\n",
    "        x=\"Value\", \n",
    "        y=\"Feature\", \n",
    "        data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:num],\n",
    "        ax=ax\n",
    "    )\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "completed-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=3, shuffle=True, random_state=239)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "secondary-vulnerability",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "views:  0.7155297482884211\n",
      "depth:  0.8440000609380154\n",
      "full_reads_percent:  0.5393429928349939\n",
      "Final score: 0.7012148154472712, median: 0.7018418003000428\n"
     ]
    }
   ],
   "source": [
    "result_dict = {\n",
    "    'views': {'r2': [], 'trees': [], 'prediction': []},\n",
    "    'full_reads_percent': {'r2': [], 'trees': [], 'prediction': []},\n",
    "    'depth': {'r2': [], 'trees': [], 'prediction': []}\n",
    "}\n",
    "for target_variable in FeaturesConstants.target:\n",
    "    FeatureGenerator = Compose(\n",
    "        transforms=[\n",
    "            DatetimeTransformer(\n",
    "                target_variable=target_variable\n",
    "            ),\n",
    "            TitleTransformer(),\n",
    "            CTRTransformer(\n",
    "                eps=(\n",
    "                    0.1 \n",
    "                    if target_variable == 'views' \n",
    "                    else 0.5\n",
    "                )\n",
    "            ),\n",
    "            TagsTransformer(target_variable=target_variable),\n",
    "            AuthorsTransformer(\n",
    "                target_variable=target_variable\n",
    "            ),\n",
    "            TextTransformer(\n",
    "                use_text_transformer=(\n",
    "                    target_variable \n",
    "                    if target_variable == 'full_reads_percent' \n",
    "                    else None\n",
    "                )\n",
    "            ),\n",
    "            NatashaTextTransformer(),\n",
    "            CategoryTransformer(),\n",
    "            FeatureSelector()\n",
    "        ]\n",
    "    )\n",
    "    train_features = FeatureGenerator(\n",
    "        data=train_data, \n",
    "        mode='train'\n",
    "    )\n",
    "    test_features = FeatureGenerator(\n",
    "        data=test_data, \n",
    "        mode='test'\n",
    "    )\n",
    "    if target_variable == 'views':\n",
    "        tfidf = TfidfVectorTransformer(\n",
    "            col='NAMES_FROM_TITLE',\n",
    "            tfidf_vectorizer=TfidfVectorizer(\n",
    "                ngram_range=(1, 4),\n",
    "                min_df=1,\n",
    "                max_features=128\n",
    "            )\n",
    "        )\n",
    "        tfidf.fit(train_data)\n",
    "        train_dd = tfidf.transform(train_data)\n",
    "        test_dd = tfidf.transform(test_data)\n",
    "        train_features = pd.concat([train_features, train_dd], axis=1)\n",
    "        test_features = pd.concat([test_features, test_dd], axis=1)\n",
    "    \n",
    "    if target_variable == 'depth':\n",
    "        tfidf = TfidfVectorTransformer(\n",
    "            col='NAMES_FROM_TITLE',\n",
    "            tfidf_vectorizer=TfidfVectorizer(\n",
    "                ngram_range=(1, 4),\n",
    "                min_df=1,\n",
    "                max_features=128\n",
    "            )\n",
    "        )\n",
    "        tfidf.fit(train_data)\n",
    "        train_dd = tfidf.transform(train_data)\n",
    "        test_dd = tfidf.transform(test_data)\n",
    "        train_features = pd.concat([train_features, train_dd], axis=1)\n",
    "        test_features = pd.concat([test_features, test_dd], axis=1)\n",
    "    if target_variable == 'full_reads_percent':\n",
    "        m = MultiLabelTransformer(col='authors_parsed')\n",
    "        m.fit(train_data)\n",
    "        train_author = m.transform(train_data)\n",
    "        test_author = m.transform(test_data)\n",
    "        train_features = pd.concat(\n",
    "            [train_features, train_author], \n",
    "            axis=1\n",
    "        )\n",
    "        test_features = pd.concat(\n",
    "            [test_features, test_author], \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "    if target_variable != 'full_reads_percent':\n",
    "        tfidf = TfidfVectorTransformer(\n",
    "            col='tags_parsed',\n",
    "            tfidf_vectorizer=TfidfVectorizer(\n",
    "                ngram_range=(1, 4) if target_variable != 'full_reads_percent' else (1, 4),\n",
    "                max_features=256 if target_variable != 'full_reads_percent' else 256,\n",
    "                min_df=5 if target_variable != 'full_reads_percent' else 5\n",
    "            )\n",
    "        )\n",
    "        tfidf.fit(train_data)\n",
    "        train_dd = tfidf.transform(train_data)\n",
    "        test_dd = tfidf.transform(test_data)\n",
    "        train_features = pd.concat([train_features, train_dd], axis=1)\n",
    "        test_features = pd.concat([test_features, test_dd], axis=1)\n",
    "        \n",
    "        cec = CountTfIdfVectorizer(\n",
    "            col='authors_parsed',\n",
    "            count_vectorizer=CountVectorizer(\n",
    "                ngram_range=(2, 2),\n",
    "                min_df=2\n",
    "            )\n",
    "        )\n",
    "        cec.fit(train_data)\n",
    "        train_features['author_popularity'] = cec.transform(train_data)\n",
    "        test_features['author_popularity'] = cec.transform(test_data)\n",
    "\n",
    "    for train_index, val_index in kfold.split(train_features):\n",
    "        X_train, X_valid = (\n",
    "            train_features.reset_index(drop=True).iloc[train_index, :], \n",
    "            train_features.reset_index(drop=True).iloc[val_index, :]\n",
    "        )\n",
    "        y_train, y_valid = (\n",
    "            y_data[target_variable].reset_index(drop=True).iloc[train_index], \n",
    "            y_data[target_variable].reset_index(drop=True).iloc[val_index]\n",
    "        )\n",
    "        if target_variable == 'views':\n",
    "            y_train = np.log(y_train)\n",
    "            y_valid = np.log(y_valid)\n",
    "        elif target_variable == 'depth':\n",
    "            y_train = np.log(np.log(y_train))\n",
    "            y_valid = np.log(np.log(y_valid))\n",
    "        else:\n",
    "            y_train = np.log(y_train)\n",
    "            y_valid = np.log(y_valid)\n",
    "                    \n",
    "        lgbm_regressor = lgb.LGBMRegressor(\n",
    "            objective='regression',\n",
    "            random_state=33,\n",
    "            early_stopping_round=100, \n",
    "            n_estimators=500,\n",
    "            subsample=1,\n",
    "            colsample_bytree=0.75,\n",
    "            learning_rate=(\n",
    "                0.08 \n",
    "                if target_variable == 'full_reads_percent' \n",
    "                else 0.08\n",
    "            ),\n",
    "            max_depth=-1,\n",
    "            verbose=-1\n",
    "        )\n",
    "        lgbm_regressor.fit(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            eval_set=[(X_valid, y_valid)], \n",
    "            eval_metric='r2', \n",
    "            verbose=-1\n",
    "        )\n",
    "\n",
    "        y_pred = lgbm_regressor.predict(X_valid)\n",
    "        r2 = r2_score(y_valid, y_pred)\n",
    "        result_dict[target_variable]['r2'].append(r2)\n",
    "        result_dict[target_variable]['trees'].append(lgbm_regressor.booster_.trees_to_dataframe())\n",
    "\n",
    "        \n",
    "        pro = lgbm_regressor.predict(test_features)\n",
    "        if target_variable == 'views':\n",
    "            pro = np.exp(pro)\n",
    "        elif target_variable == 'depth':\n",
    "            pro = np.exp(np.exp(pro))\n",
    "        else:\n",
    "            pro = np.exp(pro)\n",
    "        assert len(pro[pro < 0]) == 0\n",
    "        result_dict[target_variable]['prediction'].append(pro)\n",
    "    print(f'{target_variable}: ', np.mean(result_dict[target_variable]['r2']))\n",
    "\n",
    "full_score = (\n",
    "    np.mean(result_dict['views']['r2']) * 0.4 + \n",
    "    np.mean(result_dict['full_reads_percent']['r2']) * 0.3 + \n",
    "    np.mean(result_dict['depth']['r2']) * 0.3\n",
    ")\n",
    "full_score_2 = (\n",
    "    np.median(result_dict['views']['r2']) * 0.4 + \n",
    "    np.median(result_dict['full_reads_percent']['r2']) * 0.3 + \n",
    "    np.median(result_dict['depth']['r2']) * 0.3\n",
    ")\n",
    "print(f'Final score: {full_score}, median: {full_score_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-marriage",
   "metadata": {},
   "source": [
    "# Sub4 - best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "seven-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(\n",
    "    {\n",
    "        'document_id': test_data['document_id'].values,\n",
    "        'views': pd.DataFrame(result_dict['views']['prediction']).T.mean(axis=1),\n",
    "        'depth': pd.DataFrame(result_dict['depth']['prediction']).T.mean(axis=1),\n",
    "        'full_reads_percent': pd.DataFrame(result_dict['full_reads_percent']['prediction']).T.mean(axis=1),\n",
    "    },\n",
    ")\n",
    "sub.index = test_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "coastal-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_full = pd.concat((sub, exist_prediction)).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ambient-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_csv('../sub/sample_solution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "laughing-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_full_2 = pd.DataFrame(ss['document_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "worldwide-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "views = []\n",
    "full_reads_percent = []\n",
    "depth = []\n",
    "for j, i in sub_full_2.iterrows():\n",
    "    dummy = sub_full[sub_full['document_id'] == i['document_id']]\n",
    "    views.append(dummy['views'].values[0])\n",
    "    full_reads_percent.append(dummy['full_reads_percent'].values[0])\n",
    "    depth.append(dummy['depth'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "spare-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_full_2['views'] = views\n",
    "sub_full_2['full_reads_percent'] = full_reads_percent\n",
    "sub_full_2['depth'] = depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "general-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_full_2.to_csv(SUB_DIR / 'last3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-builder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbk",
   "language": "python",
   "name": "rbk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
